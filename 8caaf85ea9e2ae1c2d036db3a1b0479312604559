{
  "comments": [
    {
      "key": {
        "uuid": "6ab98a4d_23084eaa",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1419317
      },
      "writtenOn": "2020-09-23T13:55:47Z",
      "side": 1,
      "message": "I asked around and was told that Joey has done some changes related to encoding recently, so he\u0027s experience might help to review this.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a25fb351_f6c3b790",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1214214
      },
      "writtenOn": "2020-09-23T14:58:01Z",
      "side": 1,
      "message": "LGTM, but note that this does not match exactly what happens in Chromium. charset-free responses get some charset applied to them, and this logic is pretty complex and unintuitive. From https://github.com/GoogleChrome/lighthouse/issues/10023:\n\n\u003e Note that if UTF-8 is detected, the TLD-affiliated encoding is\n\u003e used instead for intentional misdecoding for the same reason why\n\u003e Chrome doesn\u0027t detect UTF-8: To avoid Web authors starting to\n\u003e depend on this stuff. Hence,\n\u003e https://mathiasbynens.be/demo/missing-meta-charset decodes as\n\u003e windows-1252, since .be is a windows-1252-affiliated TLD.)\n\nStill, I think falling back to utf-8 is better than not falling back at all, so LGTM.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4d320b2e_b0ac6c1c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1390564
      },
      "writtenOn": "2020-09-23T16:46:00Z",
      "side": 1,
      "message": "Debugging the tools against the site listed in the bug, it appears the string we\u0027re passing to the data-uri may have already been decoded by the time we receive it from the backend. \n\nThis actually makes me wonder if to fix this more generally we either (1) shouldn\u0027t be passing through the charset from the content-type header or (2) need to get the raw bytes to pass base64 encoded. #1 may still have problems when pages explicitly use meta charset in the content (since that will still be present), and for #2 I\u0027m not sure if we have access to the raw bytes.\n\nCurious to hear other\u0027s thoughts.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "6f805486_24c5c226",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1303398
      },
      "writtenOn": "2020-09-23T17:47:32Z",
      "side": 1,
      "message": "I think there is a lot of complicated logic going on here which could take a lot of effort to fix properly.\n\nIf you want to submit this go ahead, but I wouldn\u0027t be surprised if adding charset\u003dutf8 by default breaks other cases. Maybe at least add a comment saying that it would be better to add new cdp methods to get raw response data and the real charset chrome is using instead?",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "e6ec50a2_a5682d7c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1303398
      },
      "writtenOn": "2020-09-23T17:47:32Z",
      "side": 1,
      "message": "Yes, the content was likely already decoded using the logic here: https://source.chromium.org/chromium/chromium/src/+/master:third_party/blink/renderer/core/inspector/inspector_page_agent.cc;l\u003d232-260;drc\u003dee9e7e404e5a3f75a3ca0489aaf80490f625ca27\n\nI think that it would be helpful to have a Network.getRawResponseBody method or something which skips this decoding and always returns something base64 encoded, but then we might have to store multiple copies of resources - I thought about doing this before but I didn\u0027t end up doing it, and I don\u0027t remember exactly why.\n\nLooking at this bug, I originally thought that there was something wrong with the iframe since it should have the right \u003cmeta\u003e tag specifying the charset present in the problematic webpage, but now I\u0027m actually rather confused why adding charset\u003dutf8 to the data url actually fixes it if we already mis-decoded it before on the backend.\nDoes anyone know why this is the case?\n\nIn addition to having a Network.getRawResponseBody or something, we would also benefit here from exposing the actual encoding that chrome decided to use over cdp. Assuming that it changes over time as chrome parses the page, I\u0027m not sure if it should be a cdp event or method.\n\nI just found a bug where I talked about having a getRawResponseBody method before: https://bugs.chromium.org/p/chromium/issues/detail?id\u003d771825\nThis might be worth reconsidering and/or reopening. Maybe we could just always store the raw response data and decode it when requested by Network.getResponseBody...?",
      "parentUuid": "4d320b2e_b0ac6c1c",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "64163395_b06c0a15",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1390564
      },
      "writtenOn": "2020-09-24T17:09:58Z",
      "side": 1,
      "message": "I\u0027d guess the site in the bug works with this fix because putting utf-8 in the data-uri is taking precedence over the meta tag in the content. Though that\u0027s assuming the decoded string is ultimately being fed to the iframe as utf-8 instead of utf-16. I\u0027d expect utf-16 since that\u0027s the in-memory representation for JS strings, but we may be going through a serialization step that converts to utf-8 before the string is parsed as part of the iframe. \n\nIf that\u0027s true and consistent across resources it could potentially be \"correct\" to always pass utf-8 and ignore the content-type header. That\u0027s a big if, though.",
      "parentUuid": "e6ec50a2_a5682d7c",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}