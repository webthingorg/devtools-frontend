{
  "comments": [
    {
      "key": {
        "uuid": "6ab98a4d_23084eaa",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1419317
      },
      "writtenOn": "2020-09-23T13:55:47Z",
      "side": 1,
      "message": "I asked around and was told that Joey has done some changes related to encoding recently, so he\u0027s experience might help to review this.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "a25fb351_f6c3b790",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1214214
      },
      "writtenOn": "2020-09-23T14:58:01Z",
      "side": 1,
      "message": "LGTM, but note that this does not match exactly what happens in Chromium. charset-free responses get some charset applied to them, and this logic is pretty complex and unintuitive. From https://github.com/GoogleChrome/lighthouse/issues/10023:\n\n\u003e Note that if UTF-8 is detected, the TLD-affiliated encoding is\n\u003e used instead for intentional misdecoding for the same reason why\n\u003e Chrome doesn\u0027t detect UTF-8: To avoid Web authors starting to\n\u003e depend on this stuff. Hence,\n\u003e https://mathiasbynens.be/demo/missing-meta-charset decodes as\n\u003e windows-1252, since .be is a windows-1252-affiliated TLD.)\n\nStill, I think falling back to utf-8 is better than not falling back at all, so LGTM.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4d320b2e_b0ac6c1c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1390564
      },
      "writtenOn": "2020-09-23T16:46:00Z",
      "side": 1,
      "message": "Debugging the tools against the site listed in the bug, it appears the string we\u0027re passing to the data-uri may have already been decoded by the time we receive it from the backend. \n\nThis actually makes me wonder if to fix this more generally we either (1) shouldn\u0027t be passing through the charset from the content-type header or (2) need to get the raw bytes to pass base64 encoded. #1 may still have problems when pages explicitly use meta charset in the content (since that will still be present), and for #2 I\u0027m not sure if we have access to the raw bytes.\n\nCurious to hear other\u0027s thoughts.",
      "revId": "8caaf85ea9e2ae1c2d036db3a1b0479312604559",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba",
      "unresolved": true
    }
  ]
}